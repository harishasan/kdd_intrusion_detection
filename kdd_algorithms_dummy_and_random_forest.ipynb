{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Dataset is taken from here: https://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html.\n",
    "\n",
    "Task is to build a network intrusion detector, a predictive model capable of distinguishing between 'bad' connections, called intrusions or attacks, and 'good' normal connections. \n",
    "\n",
    "### EDA Conclusion:\n",
    "1. We found one column that has no impact on class label.\n",
    "2. We found few correlated columns which can help in reducing feature set.\n",
    "3. We found that it might be more suitable to first detect normal vs anomalous (1/0) and then predict the type of anomaly. \n",
    "4. We found that there exist no single or subset of features which alone can do the prediction. However, many features provide some level of distinction between class labels. Therefore by using all the available features together we can make a good prediction.\n",
    "5. If needed, we can make new features by combining existing features. Or we can give neural networks a try which can automatically do this for us. \n",
    "\n",
    "### Next Steps:\n",
    "To find out how accurate our data analysis has been we can do following\n",
    "1. Convert categorical features into continous or binary (1/0) features\n",
    "2. Rescale all the features between 0 and 1\n",
    "3. Using stratified samping create three sets (1) test (2) train (3) cross validation\n",
    "4. Implement a dummpy classifier and note results //http://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html\n",
    "4. Select a machine learning algorithm to apply on dataset\n",
    "5. Apply selected machine learning algorithm using all features and get the baseline accuracy of algoithm. Note the train and cross validation errors. \n",
    "6. Apply the selected machine learning algorithm using subset of features identified in EDA. Note the train and cross validation errors. \n",
    "7. Use the two step approach i.e. normal vs anomalous and then predict type of anomaly. Use all features present in feature set. Note the train and cross validation errors.\n",
    "9. Repeat previous step using the subset of features identified in EDA. Note the train and cross validation errors. \n",
    "10. Repeat steps 5-9 using a Neural network\n",
    "11. Apply the best method on test set to get final accuracy\n",
    "\n",
    "[High Bias vs High Variance] can we use train vs cross validation results to measure whether we have a high bias scenario or high variance scenario\n",
    "[At every step it is important to note two things (1) training error (2) cross validation error. It will help us in determining whether it is a high bias or high variance situation]\n",
    "Good read: https://github.com/ianozsvald/data_science_delivered/blob/master/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "#disable auto save, this sometimes hangs the browser\n",
    "%autosave 0\n",
    "import pandas as pd\n",
    "import time\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "import numpy\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import cross_validation\n",
    "from sklearn import preprocessing\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "# to supress printing of exponential notation in pandas\n",
    "pd.options.display.float_format = '{:20,.2f}'.format\n",
    "\n",
    "# avoid data truncation\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to return true if array contains binary (zero and one) values only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_only_zero_and_one(array):\n",
    "    return len(array) == 2 and ((array[0] == 0 and array[1] == 1) or ((array[0] == 1 and array[1] == 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to convert categorical features into binary\n",
    "#### use this in future instead: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html\n",
    "#### need to do this because: https://stackoverflow.com/questions/24715230/can-sklearn-random-forest-directly-handle-categorical-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# does not modify the original source\n",
    "def convert_categorical_to_binary(data, categorical_columns):\n",
    "    \n",
    "    temp_data = data.copy()\n",
    "\n",
    "    label_binarizer = []\n",
    "    for col in categorical_columns:\n",
    "        label_binarizer.append((col, sklearn.preprocessing.LabelBinarizer()))\n",
    "        \n",
    "    # df_out=True: output a data frame\n",
    "    mapper_df = DataFrameMapper(label_binarizer, df_out=True)    \n",
    "    # temp contains the new columns\n",
    "    temp = mapper_df.fit_transform(temp_data)\n",
    "    \n",
    "    # print temp[temp.isnull().any(axis=1)]\n",
    "    \n",
    "    for col in temp.columns:\n",
    "        temp_data[col] = numpy.array(temp[col])\n",
    "    \n",
    "    total_column_count = len(data.columns)\n",
    "    for col in categorical_columns:\n",
    "        total_column_count += len(data[col].unique())\n",
    "        \n",
    "    print 'new column count should be ' + str(len(temp_data.columns)) + ' and is ' + str(total_column_count)\n",
    "    return temp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to rescale all non-binary features between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# does not modify the original source\n",
    "# categorical_columns are skipped\n",
    "# if a column only has binary (0/1) values, it is skipped too\n",
    "def rescale_non_binary_columns(data, categorical_columns):\n",
    "    \n",
    "    temp_data = data.copy()\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    for col in data.columns:\n",
    "        if col not in categorical_columns and not is_only_zero_and_one(data[col].unique()):\n",
    "            # print 'scaling ' + col\n",
    "            temp_data[col] = scaler.fit_transform(temp_data[[col]])\n",
    "            \n",
    "    return temp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to print confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_confusion_matrix(confusion_matrix, labels):\n",
    "    records = len(labels)\n",
    "    for row in range(records):\n",
    "        print \"-------------\" + labels[row] + \"-------------\"\n",
    "        total = 0\n",
    "        for column in range(records):\n",
    "            total += confusion_matrix[row][column]\n",
    "        print 'total: ' + str(total)\n",
    "        print 'correct: ' + str(confusion_matrix[row][row])\n",
    "        for column in range(records):\n",
    "            if confusion_matrix[row][column] != 0 and row != column:\n",
    "                print labels[column] + ': ' + str(confusion_matrix[row][column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to print summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_summary_statistics(confusion_matrix, normal_class_index):\n",
    "    class_label_count = len(confusion_matrix)\n",
    "    total_records = 0\n",
    "    total_normal = 0\n",
    "    total_anomalous = 0\n",
    "    total_normal_correctly_identified = 0\n",
    "    total_anomalous_correctly_identified = 0\n",
    "    \n",
    "    for row in range(class_label_count):\n",
    "        for col in range(class_label_count):            \n",
    "            total_records += confusion_matrix[row][col]            \n",
    "            if row == normal_class_index:\n",
    "                total_normal += confusion_matrix[row][col]\n",
    "                if col == normal_class_index:\n",
    "                    total_normal_correctly_identified = confusion_matrix[row][col]\n",
    "            else:\n",
    "                total_anomalous += confusion_matrix[row][col]\n",
    "                if row == col:\n",
    "                    total_anomalous_correctly_identified += confusion_matrix[row][col]\n",
    "     \n",
    "    # * by 1.0 to make denominator float\n",
    "    #  If the numerator or denominator is a float, then the result will be also.\n",
    "    total_correctly_identified = total_normal_correctly_identified + total_anomalous_correctly_identified\n",
    "    correct_normal_percentage = total_normal_correctly_identified * 100/(1.0 * total_normal)\n",
    "    correct_anomalous_percentage = total_anomalous_correctly_identified * 100/(1.0 * total_anomalous)\n",
    "    correct_total_percentage = total_correctly_identified * 100/(1.0 * total_records)\n",
    "    print 'total: ' + str(total_records)\n",
    "    print 'normal: ' + str(total_normal)\n",
    "    print 'anomalous: ' + str(total_anomalous)\n",
    "    \n",
    "    print 'total correctly identified: ' + str(total_correctly_identified) + '(' + str(correct_total_percentage) + '%)'\n",
    "    print 'normal correctly identified: ' + str(total_normal_correctly_identified) + '(' + str(correct_normal_percentage) + '%)'\n",
    "    print 'anomalous correctly identified: ' + str(total_anomalous_correctly_identified) + '(' + str(correct_anomalous_percentage) + '%)'\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to print F scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_f_scores(actual_labels, predictions, unique_labels):\n",
    "    #http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "    # Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "    print 'micro: ' + str(metrics.f1_score(actual_labels, predictions, \n",
    "                                           labels=unique_labels, average='micro'))\n",
    "    # Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "    print 'macro: '+ str(metrics.f1_score(actual_labels, predictions, \n",
    "                                          labels=unique_labels, average='macro'))\n",
    "    # Calculate metrics for each label, and find their average, weighted by support (the number of true instances \n",
    "    # for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that\n",
    "    # is not between precision and recall.\n",
    "    print 'weighted: ' + str(metrics.f1_score(actual_labels, predictions, \n",
    "                                              labels=unique_labels, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv loaded\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"/Users/haris/Desktop/kdd/kdd_full.csv\")\n",
    "print \"csv loaded\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analyzing metadata\n",
    "\n",
    "### Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'rows and columns: ' + str(data.shape)\n",
    "# remove duplicate rows\n",
    "data = data.drop_duplicates()\n",
    "print 'rows and columns after removing duplicates:' + str(data.shape)\n",
    "print 'printing rows with null values'\n",
    "print len(data[data.isnull().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categorical_columns_without_label = ['service', 'flag', 'protocol_type']\n",
    "categorical_columns_with_label = ['service', 'flag', 'protocol_type', 'label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert categorical features into binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = convert_categorical_to_binary(data, categorical_columns_without_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1 for column, 0 for row\n",
    "for col in categorical_columns_without_label:\n",
    "    data = data.drop(col, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescale numeric features between 0 and 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = rescale_non_binary_columns(data, categorical_columns_with_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate features and class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = data['label']\n",
    "X = data.drop('label', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train, Cross Validation and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify = y)\n",
    "X_train, X_cross_validation, y_train, y_cross_validation = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, random_state=31, stratify = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print 'Train size: ' + str(len(y_train))\n",
    "print 'Test size: ' + str(len(y_test))\n",
    "print 'Cross validation size: ' + str(len(y_cross_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge labels and save sets on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train['label'] = y_train\n",
    "X_cross_validation['label'] = y_cross_validation\n",
    "X_test['label'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.to_csv('/Users/haris/Desktop/train.csv', sep=',', index=False)\n",
    "X_cross_validation.to_csv('/Users/haris/Desktop/cross_validation.csv', sep=',', index=False)\n",
    "X_test.to_csv('/Users/haris/Desktop/test.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Load train and cross validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csvs loaded\n",
      "644994 train rows\n",
      "214999 cross validation rows\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"/Users/haris/Desktop/kdd_datasets/train.csv\")\n",
    "cross_validation = pd.read_csv(\"/Users/haris/Desktop/kdd_datasets/cross_validation.csv\")\n",
    "print \"csvs loaded\"\n",
    "print str(len(train)) + ' train rows'\n",
    "print str(len(cross_validation)) + ' cross validation rows'\n",
    "\n",
    "y_train = train['label']\n",
    "X_train = train.drop('label', 1) \n",
    "y_cross_validation = cross_validation['label']\n",
    "X_cross_validation = cross_validation.drop('label', 1) \n",
    "labels = y_train.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Step 3: Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dummy_classfier = DummyClassifier(strategy='most_frequent', random_state=0)\n",
    "# train the classifiers\n",
    "dummy_classfier = dummy_classfier.fit(X_train, y_train)\n",
    "# test the classifiers\n",
    "dummy_classfier_predictions = dummy_classfier.predict(X_cross_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create confusion matrix from classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_cross_validation, dummy_classfier_predictions, labels=labels)\n",
    "print confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print summary of classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_summary_statistics(confusion_matrix, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix of dummy classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_confusion_matrix(confusion_matrix, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of dummy classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_f_scores(y_cross_validation, dummy_classfier_predictions, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Step 4: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_state = 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_forest_classifier = RandomForestClassifier(n_estimators=10, random_state=random_state)\n",
    "random_forest_classifier = random_forest_classifier.fit(X_train, y_train)\n",
    "random_forest_predictions = random_forest_classifier.predict(X_cross_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create confusion matrix from classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_cross_validation, random_forest_predictions, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print result summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 214999\n",
      "normal: 162563\n",
      "anomalous: 52436\n",
      "total correctly identified: 214904(99.955813748%)\n",
      "normal correctly identified: 162548(99.9907728081%)\n",
      "anomalous correctly identified: 52356(99.8474330613%)\n"
     ]
    }
   ],
   "source": [
    "print_summary_statistics(confusion_matrix, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print random forest confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------normal.-------------\n",
      "total: 162563\n",
      "correct: 162548\n",
      "ipsweep.: 6\n",
      "satan.: 3\n",
      "nmap.: 1\n",
      "warezclient.: 2\n",
      "land.: 3\n",
      "-------------ipsweep.-------------\n",
      "total: 745\n",
      "correct: 731\n",
      "normal.: 7\n",
      "nmap.: 7\n",
      "-------------neptune.-------------\n",
      "total: 48430\n",
      "correct: 48430\n",
      "-------------satan.-------------\n",
      "total: 1004\n",
      "correct: 992\n",
      "normal.: 11\n",
      "portsweep.: 1\n",
      "-------------smurf.-------------\n",
      "total: 602\n",
      "correct: 602\n",
      "-------------portsweep.-------------\n",
      "total: 713\n",
      "correct: 708\n",
      "normal.: 4\n",
      "ipsweep.: 1\n",
      "-------------nmap.-------------\n",
      "total: 311\n",
      "correct: 296\n",
      "normal.: 8\n",
      "ipsweep.: 7\n",
      "-------------teardrop.-------------\n",
      "total: 183\n",
      "correct: 182\n",
      "satan.: 1\n",
      "-------------back.-------------\n",
      "total: 194\n",
      "correct: 194\n",
      "-------------warezclient.-------------\n",
      "total: 178\n",
      "correct: 162\n",
      "normal.: 16\n",
      "-------------guess_passwd.-------------\n",
      "total: 10\n",
      "correct: 9\n",
      "land.: 1\n",
      "-------------imap.-------------\n",
      "total: 2\n",
      "correct: 1\n",
      "normal.: 1\n",
      "-------------pod.-------------\n",
      "total: 41\n",
      "correct: 40\n",
      "normal.: 1\n",
      "-------------buffer_overflow.-------------\n",
      "total: 6\n",
      "correct: 2\n",
      "normal.: 4\n",
      "-------------land.-------------\n",
      "total: 4\n",
      "correct: 1\n",
      "normal.: 3\n",
      "-------------phf.-------------\n",
      "total: 1\n",
      "correct: 1\n",
      "-------------loadmodule.-------------\n",
      "total: 2\n",
      "correct: 1\n",
      "multihop.: 1\n",
      "-------------multihop.-------------\n",
      "total: 1\n",
      "correct: 0\n",
      "normal.: 1\n",
      "-------------warezmaster.-------------\n",
      "total: 4\n",
      "correct: 3\n",
      "normal.: 1\n",
      "-------------ftp_write.-------------\n",
      "total: 2\n",
      "correct: 0\n",
      "normal.: 1\n",
      "warezclient.: 1\n",
      "-------------rootkit.-------------\n",
      "total: 2\n",
      "correct: 0\n",
      "normal.: 2\n",
      "-------------perl.-------------\n",
      "total: 1\n",
      "correct: 1\n",
      "-------------spy.-------------\n",
      "total: 0\n",
      "correct: 0\n"
     ]
    }
   ],
   "source": [
    "print_confusion_matrix(confusion_matrix, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro: 0.99955813748\n",
      "macro: 0.726997715881\n",
      "weighted: 0.999541505154\n"
     ]
    }
   ],
   "source": [
    "print_f_scores(y_cross_validation, random_forest_predictions, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Remove the columns identified redundant/useless in EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(644994, 122)\n",
      "(214999, 122)\n",
      "(644994, 120)\n",
      "(214999, 120)\n"
     ]
    }
   ],
   "source": [
    "columns_to_remove = ['num_outbound_cmds', 'num_root']\n",
    "X_train_copy = X_train.copy()\n",
    "X_cross_validation_copy = X_cross_validation.copy()\n",
    "\n",
    "print X_train_copy.shape\n",
    "print X_cross_validation_copy.shape\n",
    "\n",
    "for col in columns_to_remove:\n",
    "    X_train_copy.drop(col, axis=1, inplace=True)\n",
    "    X_cross_validation_copy.drop(col, axis=1, inplace=True)\n",
    "\n",
    "print X_train_copy.shape\n",
    "print X_cross_validation_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_forest_classifier = RandomForestClassifier(n_estimators=10, random_state=random_state)\n",
    "random_forest_classifier = random_forest_classifier.fit(X_train_copy, y_train)\n",
    "random_forest_predictions = random_forest_classifier.predict(X_cross_validation_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_cross_validation, random_forest_predictions, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 214999\n",
      "normal: 162563\n",
      "anomalous: 52436\n",
      "total correctly identified: 214885(99.9469764976%)\n",
      "normal correctly identified: 162541(99.9864667852%)\n",
      "anomalous correctly identified: 52344(99.8245480204%)\n"
     ]
    }
   ],
   "source": [
    "print_summary_statistics(confusion_matrix, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results were same when we removed the feature with 0 variance [num_outbound_cmds] probably because random forest itself would have discarded this feature internally\n",
    "#### Results almost remained same (15 less correct classifications) when we removed one highly correlated featuere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying different number of trees (10-90) on random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random state: 37\n",
      "----------------------------------------\n",
      "n_estimators: 10\n",
      "total: 214999\n",
      "normal: 162563\n",
      "anomalous: 52436\n",
      "total correctly identified: 214904(99.955813748%)\n",
      "normal correctly identified: 162548(99.9907728081%)\n",
      "anomalous correctly identified: 52356(99.8474330613%)\n",
      "total time: 19.2350928783 seconds\n",
      "----------------------------------------\n",
      "n_estimators: 20\n",
      "total: 214999\n",
      "normal: 162563\n",
      "anomalous: 52436\n",
      "total correctly identified: 214894(99.9511625635%)\n",
      "normal correctly identified: 162547(99.990157662%)\n",
      "anomalous correctly identified: 52347(99.8302692806%)\n",
      "total time: 37.6239840984 seconds\n",
      "----------------------------------------\n",
      "n_estimators: 30\n",
      "total: 214999\n",
      "normal: 162563\n",
      "anomalous: 52436\n",
      "total correctly identified: 214897(99.9525579189%)\n",
      "normal correctly identified: 162545(99.9889273697%)\n",
      "anomalous correctly identified: 52352(99.8398047143%)\n",
      "total time: 54.3900029659 seconds\n",
      "----------------------------------------\n",
      "n_estimators: 40\n",
      "total: 214999\n",
      "normal: 162563\n",
      "anomalous: 52436\n",
      "total correctly identified: 214900(99.9539532742%)\n",
      "normal correctly identified: 162547(99.990157662%)\n",
      "anomalous correctly identified: 52353(99.8417118011%)\n",
      "total time: 68.6285791397 seconds\n",
      "----------------------------------------\n",
      "n_estimators: 50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-4a6f33844901>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'n_estimators: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mrandom_forest_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mrandom_forest_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_forest_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mrandom_forest_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_forest_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_cross_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mconfusion_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_cross_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_forest_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/haris/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    324\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 326\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/haris/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/haris/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/haris/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/haris/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/haris/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/haris/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/haris/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/haris/anaconda/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    740\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/haris/anaconda/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    348\u001b[0m                                            self.min_impurity_split)\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "random_state = 37\n",
    "print 'random state: ' + str(random_state)\n",
    "\n",
    "for num in range(1, 10):\n",
    "    print '----------------------------------------'\n",
    "    start_time = time.time()\n",
    "    n_estimators = num * 10\n",
    "    print 'n_estimators: ' + str(n_estimators)\n",
    "    random_forest_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)\n",
    "    random_forest_classifier = random_forest_classifier.fit(X_train, y_train)\n",
    "    random_forest_predictions = random_forest_classifier.predict(X_cross_validation)\n",
    "    confusion_matrix = metrics.confusion_matrix(y_cross_validation, random_forest_predictions, labels=labels)\n",
    "    print_summary_statistics(confusion_matrix, 0)\n",
    "    total_time = time.time() - start_time\n",
    "    print 'total time: ' + str(total_time) + ' seconds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: no impact on number of correct outcomes of number of trees in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating anomalous and normal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labels: normal.' 'labels: anomalous']\n"
     ]
    }
   ],
   "source": [
    "train_copy = train.copy()\n",
    "cross_validation_copy = cross_validation.copy()\n",
    "\n",
    "train_copy.loc[train_copy['label'] != 'normal.', 'label'] = 'anomalous'\n",
    "cross_validation_copy.loc[cross_validation_copy['label'] != 'normal.', 'label'] = 'anomalous'\n",
    "\n",
    "y_train_copy = train_copy['label']\n",
    "X_train_copy = train_copy.drop('label', 1) \n",
    "y_cross_validation_copy = cross_validation_copy['label']\n",
    "X_cross_validation_copy = cross_validation_copy.drop('label', 1) \n",
    "\n",
    "labels = y_train_copy.unique()\n",
    "print 'labels: ' + labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 214999\n",
      "normal: 162563\n",
      "anomalous: 52436\n",
      "total correctly identified: 214925(99.9655812353%)\n",
      "normal correctly identified: 162539(99.9852364929%)\n",
      "anomalous correctly identified: 52386(99.9046456633%)\n"
     ]
    }
   ],
   "source": [
    "random_forest_classifier = RandomForestClassifier(n_estimators=10, random_state=random_state)\n",
    "random_forest_classifier = random_forest_classifier.fit(X_train_copy, y_train_copy)\n",
    "random_forest_predictions = random_forest_classifier.predict(X_cross_validation_copy)\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_cross_validation_copy, random_forest_predictions, labels=labels)\n",
    "print_summary_statistics(confusion_matrix, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: by first doing anomalous vs normal classification our correct predictions overall increased by 26 and our correct prediction of anomalous increased by 36.\n",
    "\n",
    "#### also tried this approach by removing columns identified useless/redundant in EDA but no benefit found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-94-12e885848aae>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-94-12e885848aae>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    total_time = time..time() - start_time\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print 'random state: ' + str(random_state)\n",
    "\n",
    "for num in range(1, 10):\n",
    "    print '----------------------------------------'\n",
    "    start_time = time.time()\n",
    "    n_estimators = num * 10\n",
    "    print 'n_estimators: ' + str(n_estimators)\n",
    "    random_forest_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)\n",
    "    random_forest_classifier = random_forest_classifier.fit(X_train_copy, y_train_copy)\n",
    "    random_forest_predictions = random_forest_classifier.predict(X_cross_validation_copy)\n",
    "    confusion_matrix = metrics.confusion_matrix(y_cross_validation_copy, random_forest_predictions, labels=labels)\n",
    "    print_summary_statistics(confusion_matrix, 0)\n",
    "    total_time = time.time() - start_time\n",
    "    print 'total time: ' + str(total_time) + ' seconds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: even in case of binary classification (normal vs anomalous) increasing the number of trees didn't help\n",
    "\n",
    "### Building complete binary classifier using Random Forest\n",
    "\n",
    "#### Make two sets (1) normal vs abnormal (2) all anomalous\n",
    "#### First detect normal vs anomalous (1/0), if anomalous, use the second classifier to predict type of anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get copy of train and cross validation data\n",
    "train_data = train.copy()\n",
    "cross_validation_data = cross_validation.copy()\n",
    "\n",
    "# create a new feature binary_label and set it to anomalous/binary\n",
    "train_data['binary_label'] = 'normal.'\n",
    "cross_validation_data['binary_label'] = 'normal.'\n",
    "train_data.loc[train_data['label'] != 'normal.', 'binary_label'] = 'anomalous'\n",
    "cross_validation_data.loc[cross_validation_data['label'] != 'normal.', 'binary_label'] = 'anomalous'\n",
    "\n",
    "# separate, X, y_label and y_binary_label for train data\n",
    "y_train_label = train_data['label']\n",
    "y_train_binary_label = train_data['binary_label']\n",
    "X_train_data = train_data.drop(['label', 'binary_label'], 1) \n",
    "\n",
    "# separate anomalous data and labels for train\n",
    "X_train_anomalous = X_train_data.iloc[y_train_label[y_train_label != 'normal.'].index.tolist()]\n",
    "y_train_anomalous = y_train_label.iloc[y_train_label[y_train_label != 'normal.'].index.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train two classifiers (1) binary (2) anomalous and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train binary classifier\n",
    "random_forest_binary_classifier = RandomForestClassifier(n_estimators=10, random_state=37)\n",
    "random_forest_binary_classifier = random_forest_binary_classifier.fit(X_train_data, y_train_binary_label)\n",
    "# train anomalous classifier\n",
    "random_forest_anomalous_classifier = RandomForestClassifier(n_estimators=10, random_state=37)\n",
    "random_forest_anomalous_classifier = random_forest_anomalous_classifier.fit(X_train_anomalous, y_train_anomalous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare cross validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# separate, X, y_label and y_binary_label for cross validation data\n",
    "y_cross_validation_label = cross_validation_data['label']\n",
    "y_cross_validation_binary_label = cross_validation_data['binary_label']\n",
    "X_cross_validation_data = cross_validation_data.drop(['label', 'binary_label'], 1) \n",
    "\n",
    "# separate anomalous data and labels for cross_validation\n",
    "# this will help us in determining the accuracy of anomalous only classifier\n",
    "X_cross_validation_anomalous = X_cross_validation_data.iloc[y_cross_validation_label[y_cross_validation_label \n",
    "                                                                                     != 'normal.'].index.tolist()]\n",
    "y_cross_vadliation_anomalous = y_cross_validation_label.iloc[y_cross_validation_label[y_cross_validation_label \n",
    "                                                                                      != 'normal.'].index.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Classifier Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro: 0.999655812353\n",
      "macro: 0.999533308529\n",
      "weighted: 0.999655783437\n"
     ]
    }
   ],
   "source": [
    "binary_unique_labels = y_train_binary_label.unique()\n",
    "\n",
    "random_forest_binary_predictions = random_forest_binary_classifier.predict(X_cross_validation_data)\n",
    "confusion_matrix_binary_classifier = metrics.confusion_matrix(y_cross_validation_binary_label, \n",
    "                                                              random_forest_binary_predictions,\n",
    "                                                              labels=binary_unique_labels)\n",
    "\n",
    "print_f_scores(y_cross_validation_binary_label, random_forest_binary_predictions, binary_unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anomalous Classifier Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro: 0.999389732245\n",
      "macro: 0.750320468047\n",
      "weighted: 0.999352475034\n"
     ]
    }
   ],
   "source": [
    "anomalous_labels_unique = y_train_anomalous.unique()\n",
    "\n",
    "random_forest_anomalous_predictions = random_forest_anomalous_classifier.predict(X_cross_validation_anomalous)\n",
    "confusion_matrix_anomalous_classifier = metrics.confusion_matrix(y_cross_vadliation_anomalous, \n",
    "                                                              random_forest_anomalous_predictions,\n",
    "                                                              labels=anomalous_labels_unique)\n",
    "\n",
    "print_f_scores(y_cross_vadliation_anomalous, random_forest_anomalous_predictions, anomalous_labels_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combined accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro: 0.999548835111\n",
      "macro: 0.662630371032\n",
      "weighted: 0.99953377716\n",
      "total: 214999\n",
      "normal: 162563\n",
      "anomalous: 52436\n",
      "total correctly identified: 214902(99.9548835111%)\n",
      "normal correctly identified: 162539(99.9852364929%)\n",
      "anomalous correctly identified: 52363(99.8607826684%)\n"
     ]
    }
   ],
   "source": [
    "# we have all the binary predictions in the 'random_forest_binary_predictions'\n",
    "# find index of all anomalous predictions within 'random_forest_binary_predictions'\n",
    "# because we will use anomalous classifier to predict exact type of anomaly\n",
    "X_cross_validation_of_anomalous_predictions = X_cross_validation.iloc[numpy.where(random_forest_binary_predictions\n",
    "                                                                               != 'normal.')]\n",
    "#y_cross_validation_of_anomalous_predictions = y_cross_validation_label.iloc[numpy.where(random_forest_binary_predictions\n",
    "#                                                                               != 'normal.')]\n",
    "\n",
    "# make the anomalous predictions\n",
    "cross_validation_anomalous_predictions = random_forest_anomalous_classifier.predict(\n",
    "    X_cross_validation_of_anomalous_predictions)\n",
    "\n",
    "# merge the anomalous predictions back in 'random_forest_binary_predictions' to \n",
    "# get final accuracy\n",
    "anomalous_predictions_index = 0\n",
    "for index in range(0, len(random_forest_binary_predictions)):\n",
    "    if random_forest_binary_predictions[index] != 'normal.':\n",
    "        random_forest_binary_predictions[index] = cross_validation_anomalous_predictions[anomalous_predictions_index]\n",
    "        anomalous_predictions_index += 1\n",
    "\n",
    "all_unique_labels = pd.Series.unique(y_train_label)\n",
    "# random_forest_binary_predictions now contains all predictions\n",
    "confusion_matrix_complete = metrics.confusion_matrix(y_cross_validation_label, random_forest_binary_predictions, \n",
    "                                            labels=all_unique_labels)\n",
    "\n",
    "# print the final score of cross validation data\n",
    "print_f_scores(y_cross_validation_label, random_forest_binary_predictions, all_unique_labels)\n",
    "print_summary_statistics(confusion_matrix_complete, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion on random forest: Binary method is slightly better in predicting anomalous records. It predicted (7) less false positives than normal approach. \n",
    "\n",
    "## Accuracy of final random forest on test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv loaded\n",
      "644994 test rows\n",
      "micro: 0.999581393402\n",
      "macro: 0.625597103083\n",
      "weighted: 0.999560347679\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(\"/Users/haris/Desktop/kdd_datasets/test.csv\")\n",
    "print \"csv loaded\"\n",
    "print str(len(test)) + ' test rows'\n",
    "\n",
    "test['binary_label'] = 'normal.'\n",
    "test.loc[test['label'] != 'normal.', 'binary_label'] = 'anomalous'\n",
    "\n",
    "# separate, X, y_label and y_binary_label for cross validation data\n",
    "y_test_label = test['label']\n",
    "y_test_binary_label = test['binary_label']\n",
    "X_test = test.drop(['label', 'binary_label'], 1) \n",
    "random_forest_test_binary_predictions = random_forest_binary_classifier.predict(X_test)\n",
    "\n",
    "# find index of all anomalous predictions\n",
    "X_test_of_anomalous_predictions = X_test.iloc[numpy.where(random_forest_test_binary_predictions\n",
    "                                                                               != 'normal.')]\n",
    "y_test_of_anomalous_predictions = y_test_label.iloc[numpy.where(random_forest_test_binary_predictions\n",
    "                                                                               != 'normal.')]\n",
    "\n",
    "test_anomalous_predictions = random_forest_anomalous_classifier.predict(\n",
    "    X_test_of_anomalous_predictions)\n",
    "\n",
    "test_anomalous_predictions_index = 0\n",
    "for index in range(0, len(random_forest_test_binary_predictions)):\n",
    "    if random_forest_test_binary_predictions[index] != 'normal.':\n",
    "        random_forest_test_binary_predictions[index] = test_anomalous_predictions[test_anomalous_predictions_index]\n",
    "        test_anomalous_predictions_index += 1\n",
    "\n",
    "print_f_scores(y_test_label, random_forest_test_binary_predictions, all_unique_labels)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
